{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ad12c61-0537-4f9c-a840-0ab845954a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dict1 = {\"Roll No\":[1, 2,3,4,5,6],\n",
    "         \"Name\" :[\"Ankush\",\"Shantanu\",\"Swara\",\"Siddhi\",\"Anurag\",\"Shivam\"],\n",
    "         \"Eng_Marks\": [79,58,76,np.nan,87,65],\n",
    "         \"Science_Marks\":[89,45,76,np.nan,np.nan,67], \n",
    "         \"Maths_Marks\":[76,54,34,77,np.nan,65],\n",
    "         \"History_Marks\": [54,np.nan,76,np.nan,77,90],\n",
    "         \"Geography_Marks\":[78,43,np.nan,65,77,60]\n",
    "        }\n",
    "\n",
    "ds = pd.DataFrame(dict1)\n",
    "\n",
    "ds\n",
    "\n",
    "ds.isnull().sum()\n",
    "\n",
    "ds.info()\n",
    "\n",
    "boolean = ds.notnull()\n",
    "boolean\n",
    "\n",
    "Fill NAN values with ZERO\n",
    "\n",
    "#replace NaN values with Zero\n",
    "ds.fillna(0,inplace=True)\n",
    "ds\n",
    "\n",
    "#fill with next values\n",
    "ds1 = pd.DataFrame(dict1)\n",
    "ds1.fillna(method =\"pad\",inplace=True)\n",
    "ds1\n",
    "\n",
    "ds2 = pd.DataFrame(dict1)\n",
    "\n",
    "#fill with previoys values\n",
    "ds2.fillna(method=\"bfill\")\n",
    "\n",
    "#fill null values with mean\n",
    "ds2[\"Eng_Marks\"].fillna(int(ds2[\"Eng_Marks\"].mean()),inplace =True)\n",
    "ds2[\"Science_Marks\"].fillna(int(ds2[\"Science_Marks\"].mean()),inplace =True)\n",
    "ds2[\"Maths_Marks\"].fillna(int(ds2[\"Maths_Marks\"].mean()),inplace =True)\n",
    "ds2[\"History_Marks\"].fillna(int(ds2[\"History_Marks\"].mean()),inplace =True)\n",
    "ds2[\"Geography_Marks\"].fillna(int(ds2[\"Geography_Marks\"].mean()),inplace =True)\n",
    "\n",
    "ds2\n",
    "\n",
    "ds3 = pd.DataFrame(dict1)\n",
    "ds3\n",
    "\n",
    "ds3.replace(to_replace = np.nan,value=85)\n",
    "\n",
    "ds3.interpolate(method=\"linear\", limit_direction=\"forward\")\n",
    "\n",
    "\n",
    "dss=ds3.dropna(how =\"all\")\n",
    "dss\n",
    "\n",
    "ds3.dropna(axis = 1)\n",
    "\n",
    "ds3.dropna(axis = 0)\n",
    "\n",
    "OUTLIERS\n",
    "\n",
    "\n",
    "#importing\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load dataset\n",
    "bos_hou = load_boston()\n",
    "\n",
    "#create the dataframe\n",
    "df_boston = pd.DataFrame(bos_hou.data)\n",
    "column_name = bos_hou.feature_names\n",
    "df_boston.columns = column_name\n",
    "df_boston.head()\n",
    "\n",
    "#visualization\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(df_boston['DIS'])\n",
    "\n",
    "# Position \n",
    "print(np.where(df_boston['DIS']>10))\n",
    "\n",
    "Scattered plot\n",
    "\n",
    "df_boston.info()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "ax.scatter(df_boston['INDUS'],df_boston['TAX'])\n",
    "\n",
    "ax.set_xlabel('(Proportion non retail business acres)/(town)')\n",
    "\n",
    "ax.set_ylabel('(Full value property tax rate)/($10,000)')\n",
    "plt.show()\n",
    "\n",
    "print(np.where((df_boston['INDUS']>20)&(df_boston['TAX']>600)))\n",
    "\n",
    "#Z SCore\n",
    "\n",
    "# Z score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    " \n",
    "z = np.abs(stats.zscore(df_boston['DIS']))\n",
    "print(z)\n",
    "\n",
    "threshold = 3\n",
    "\n",
    "#position of the outlier\n",
    "print(np.where(z>3))\n",
    "\n",
    "\n",
    "#IQR \n",
    "\n",
    "\n",
    "#IQR\n",
    "Q1 = np.percentile(df_boston['DIS'],25,interpolation = 'midpoint')\n",
    "Q3 = np.percentile(df_boston['DIS'],75,interpolation = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "IQR\n",
    "\n",
    "# Above Upper bound\n",
    "upper = df_boston['DIS'] >= (Q3+1.5*IQR)\n",
    " \n",
    "print(\"Upper bound:\",upper)\n",
    "print(np.where(upper))\n",
    " \n",
    "# Below Lower bound\n",
    "lower = df_boston['DIS'] <= (Q1-1.5*IQR)\n",
    "print(\"Lower bound:\", lower)\n",
    "print(np.where(lower))\n",
    "\n",
    "#REMOVE THE OUTLIERS\n",
    "\n",
    "\n",
    "# Importing\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "bos_hou = load_boston()\n",
    "\n",
    "# Create the dataframe\n",
    "column_name = bos_hou.feature_names\n",
    "df_boston = pd.DataFrame(bos_hou.data)\n",
    "df_boston.columns = column_name\n",
    "df_boston.head()\n",
    "\n",
    "''' Detection '''\n",
    "# IQR\n",
    "Q1 = np.percentile(df_boston['DIS'], 25,interpolation = 'midpoint')\n",
    "\n",
    "Q3 = np.percentile(df_boston['DIS'], 75,interpolation = 'midpoint')\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Old Shape: \", df_boston.shape)\n",
    "\n",
    "# Upper bound\n",
    "upper = np.where(df_boston['DIS'] >= (Q3+1.5*IQR))\n",
    "# Lower bound\n",
    "lower = np.where(df_boston['DIS'] <= (Q1-1.5*IQR))\n",
    "\n",
    "''' Removing the Outliers '''\n",
    "df_boston.drop(upper[0], inplace = True)\n",
    "df_boston.drop(lower[0], inplace = True)\n",
    "\n",
    "print(\"New Shape: \", df_boston.shape)\n",
    "\n",
    "#DATA TRANSFORMATION\n",
    "\n",
    "\n",
    "# importing pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Creating the DataFrame\n",
    "df = pd.DataFrame({\"A\":[12, 4, 5, None, 1],\"B\":[7, 2, 54, 3, None],\"C\":[20, 16, 11, 3, 8], \"D\":[14, 3, None, 2, 6]})\n",
    "\n",
    "# Create the index\n",
    "index_ = ['Row_1', 'Row_2', 'Row_3', 'Row_4', 'Row_5']\n",
    "\n",
    "# Set the index\n",
    "df.index = index_\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "# pass a list of functions\n",
    "result = df.transform(func = ['sqrt', 'exp'])\n",
    "result\n",
    "\n",
    "#Scaling\n",
    "\n",
    "\n",
    "#M1 : Using pandas and numpy\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = sns.load_dataset('iris')\n",
    "print('Original Dataset')\n",
    "data.head()\n",
    "\n",
    "# Min-Max Normalization\n",
    "df = data.drop('species', axis=1)\n",
    "df_norm = (df-df.min())/(df.max()-df.min())\n",
    "df_norm = pd.concat((df_norm, data.species))\n",
    "\n",
    "print(\"Scaled Dataset Using Pandas\")\n",
    "df_norm.head()\n",
    "\n",
    "#load the dataset\n",
    "bos_hou = load_boston()\n",
    "\n",
    "##create the dataframe\n",
    "column_name = bos_hou.feature_names\n",
    "df_boston = pd.DataFrame(bos_hou.data)\n",
    "df_boston.columns = column_name\n",
    "df_boston.head()\n",
    "\n",
    "# skewness along the index axis\n",
    "df_boston.skew(axis = 0, skipna = True)\n",
    "\n",
    "# find skewness in each row\n",
    "df.skew(axis = 1, skipna = True)\n",
    "\n",
    "#DISTRIBUTION\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import statistics\n",
    "  \n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "mean = statistics.mean(df_norm[\"sepal_length\"])\n",
    "sd = statistics.stdev(df_norm[\"sepal_length\"])\n",
    "  \n",
    "plt.plot(df_norm[\"sepal_length\"], norm.pdf(df_norm[\"sepal_length\"], mean, sd))\n",
    "plt.show()\n",
    "print(mean)\n",
    "print(sd)\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "mean = statistics.mean(df_norm[\"petal_width\"])\n",
    "sd = statistics.stdev(df_norm[\"petal_width\"])\n",
    "  \n",
    "plt.plot(df_norm[\"petal_width\"], norm.pdf(df_norm[\"petal_width\"], mean, sd))\n",
    "plt.show()\n",
    "print(mean)\n",
    "print(sd)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = sns.load_dataset('iris')\n",
    "print('Original Dataset')\n",
    "data.head()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_scaled = scaler.fit_transform(df.to_numpy())\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[\n",
    "'sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "\n",
    "print(\"Scaled Dataset Using MinMaxScaler\")\n",
    "df_scaled.head()\n",
    "\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "mean = statistics.mean(df_scaled[\"sepal_length\"])\n",
    "sd = statistics.stdev(df_scaled[\"sepal_length\"])\n",
    "  \n",
    "plt.plot(df_scaled[\"sepal_length\"], norm.pdf(df_scaled[\"sepal_length\"], mean, sd))\n",
    "plt.show()\n",
    "print(mean)\n",
    "print(sd)\n",
    "\n",
    "#STANDRDIZATION\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset('iris')\n",
    "print('Original Dataset')\n",
    "data.head()\n",
    "\n",
    "data.drop([\"species\"],inplace=True,axis=1)\n",
    "\n",
    "data\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "df_scaled = std_scaler.fit_transform(data.to_numpy())\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[\n",
    "'sepal_length','sepal_width','petal_length','petal_width'])\n",
    "\n",
    "print(\"Scaled Dataset Using StandardScaler\")\n",
    "df_scaled.head()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
